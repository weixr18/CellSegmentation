{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as tfs\n",
    "import cv2\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "def load_images(file_names):\n",
    "    images = []\n",
    "    for file_name in file_names:\n",
    "        img = cv2.imread(file_name, -1)\n",
    "        img = img_standardization(img)\n",
    "        images.append(img)\n",
    "    images = np.array(images)\n",
    "    return images\n",
    "\n",
    "\n",
    "def unit16b2uint8(img):\n",
    "    if img.dtype == 'uint8':\n",
    "        return img\n",
    "    elif img.dtype == 'uint16':\n",
    "        return img.astype(np.uint8)\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            'No such of img transfer type: {} for img'.format(img.dtype))\n",
    "\n",
    "\n",
    "def img_standardization(img):\n",
    "    img = unit16b2uint8(img)\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 2:\n",
    "        img = np.expand_dims(img, 2)\n",
    "        img = np.tile(img, (1, 1, 3))\n",
    "        return img\n",
    "    elif len(img.shape) == 3:\n",
    "        return img\n",
    "    else:\n",
    "        raise TypeError('The Depth of image large than 3 \\n')\n",
    "    \"\"\"\n",
    "    return img\n",
    "\n",
    "\n",
    "def binaryzation(image):\n",
    "    image[image > 0] = 1\n",
    "    return image\n",
    "\n",
    "\n",
    "SHOW_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(width_in, height_in, width_out, height_out):\n",
    "\n",
    "    # get train x\n",
    "    # absp = '/'.join(os.path.abspath(__file__).split('\\\\')[:-1]) + '/'\n",
    "    train_x_path = 'C:/me/dataset1/train/'\n",
    "    train_x_list = [os.path.join(train_x_path, image)\n",
    "                    for image in os.listdir(train_x_path)]\n",
    "    train_X = load_images(train_x_list)\n",
    "\n",
    "    # get train y\n",
    "    train_y_path = 'C:/me/dataset1/train_GT/SEG'\n",
    "    train_y_list = [os.path.join(train_y_path, image)\n",
    "                    for image in os.listdir(train_y_path)]\n",
    "    train_Y = load_images(train_y_list)\n",
    "    \n",
    "\n",
    "    if SHOW_DATA:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(train_x[3])\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(train_y[3] * 255)\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\"\n",
    "    result_path = absp + '../supplementary/dataset1/test_RES'\n",
    "    if not os.path.exists(result_path):\n",
    "        os.mkdir(result_path)\n",
    "    \"\"\"\n",
    "\n",
    "    train_X, test_X, train_y, test_y = train_test_split(train_X, train_Y,\n",
    "                                                        test_size=0.1,\n",
    "                                                        random_state=0)\n",
    "    \n",
    "    train_y = binaryzation(train_y)  # for instance segmentation\n",
    "\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "width_in = 628\n",
    "height_in = 628\n",
    "width_out = 628\n",
    "height_out = 628\n",
    "\n",
    "x_train, y_train, x_val, y_val = get_dataset(\n",
    "    width_in, height_in, width_out, height_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 0, 34)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].min(), y_train[0].max(), y_val[0].min(), y_val[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((157, 628, 628), (18, 628, 628))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input augmentation\n",
    "im_aug = tfs.Compose([\n",
    "    tfs.RandomResizedCrop((width_in, height_in)),\n",
    "    tfs.RandomHorizontalFlip(),\n",
    "    tfs.RandomVerticalFlip(),\n",
    "    tfs.RandomCrop(width_in)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(\n",
    "                scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=1, n_classes=2, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 32)\n",
    "        self.down1 = Down(32, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        self.down4 = Down(256, 256)\n",
    "        self.up1 = Up(512, 128, bilinear)\n",
    "        self.up2 = Up(256, 64, bilinear)\n",
    "        self.up3 = Up(128, 32, bilinear)\n",
    "        self.up4 = Up(64, 32, bilinear)\n",
    "        self.outpred = OutConv(32, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x/255.0\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outpred(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "\n",
    "unet = UNet(n_channels=1, n_classes=2,)\n",
    "if use_gpu:\n",
    "    unet = unet.cuda()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(unet.parameters(), lr=0.01, momentum=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3351714\n"
     ]
    }
   ],
   "source": [
    "size_all = 0\n",
    "for name,parameters in unet.named_parameters():\n",
    "    size_all += torch.Size.numel(parameters.size())\n",
    "    # print(name,':',parameters.size())\n",
    "print(size_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, labels, optimizer, criterion, unet, width_out, height_out):\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = unet(inputs)\n",
    "    # outputs.shape =(batch_size, n_classes, img_cols, img_rows)\n",
    "    outputs = outputs.permute(0, 2, 3, 1)\n",
    "    # outputs.shape =(batch_size, img_cols, img_rows, n_classes)\n",
    "    m = outputs.shape[0]\n",
    "    outputs = outputs.reshape(m*width_out*height_out, 2)\n",
    "    labels = labels.reshape(m*width_out*height_out)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_loss(x_val, y_val, width_out, height_out,\n",
    "                 unet, batch_size=1, use_gpu=True):\n",
    "\n",
    "    epoch_iter = np.ceil(x_val.shape[0] / batch_size).astype(int)\n",
    "\n",
    "    j_scores = []\n",
    "\n",
    "    predict_ys = []\n",
    "    for i in range(epoch_iter):\n",
    "        # preprocess\n",
    "        batch_val_x = torch.from_numpy(\n",
    "            x_val[i*batch_size:(i + 1)*batch_size]).float()\n",
    "        if (len(batch_val_x.size()) == 3):\n",
    "            batch_val_x = batch_val_x.unsqueeze(1)\n",
    "        if use_gpu:\n",
    "            batch_val_x = batch_val_x.cuda()\n",
    "\n",
    "        # get predict\n",
    "        batch_predict_y = unet(batch_val_x)\n",
    "        batch_predict_y = batch_predict_y.cpu().detach().numpy()\n",
    "        predict_ys.append(batch_predict_y)\n",
    "\n",
    "    predict_ys = np.array(predict_ys)\n",
    "    shape = predict_ys.shape\n",
    "    # print(\"predict_ys.shape\", predict_ys.shape)\n",
    "    predict_ys = np.reshape(\n",
    "        predict_ys, (shape[0]*shape[1], shape[2], shape[3], shape[4]))\n",
    "\n",
    "    # post process\n",
    "    predict_ys = post_process(predict_ys)\n",
    "\n",
    "    # get GT\n",
    "    gt_ys = y_val\n",
    "\n",
    "    if False :\n",
    "        show_pic(x_val[0], gt_ys[0], predict_ys[0])\n",
    "\n",
    "    # calc jaccard score\n",
    "    for j in range(len(predict_ys)):\n",
    "        j_score = calc_jaccard(predict_ys[j], gt_ys[j])\n",
    "        j_scores.append(j_score)\n",
    "\n",
    "    j_score = np.mean(j_scores)\n",
    "    return j_score\n",
    "\n",
    "\n",
    "def post_process(batch_predict_y):\n",
    "    \"\"\"post process of the result\"\"\"\n",
    "    # shape: [batch_size, 2, width, height]\n",
    "\n",
    "    batch_predict_y = batch_predict_y[:, 1, :, :]\n",
    "\n",
    "    res = []\n",
    "    for predict_y in batch_predict_y:\n",
    "        # binarization\n",
    "        predict_y[predict_y > 0] = 1\n",
    "        predict_y[predict_y <= 0] = 0\n",
    "\n",
    "        # open\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\n",
    "        predict_y = cv2.erode(predict_y, kernel)  # 腐蚀\n",
    "        predict_y = cv2.dilate(predict_y, kernel)  # 膨胀\n",
    "\n",
    "        # parse\n",
    "        predict_y = predict_y.astype(np.uint8) * 255\n",
    "        __, contours, _ = cv2.findContours(\n",
    "            predict_y, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)  # 寻找连通域\n",
    "\n",
    "        areas = [cv2.contourArea(cnt) for cnt in contours]\n",
    "        cellIndexs = np.argsort(areas)\n",
    "\n",
    "        predict_y = np.zeros([predict_y.shape[0], predict_y.shape[1]])\n",
    "        for j in range(len(cellIndexs)):\n",
    "            cv2.drawContours(predict_y, contours, j, j, cv2.FILLED)\n",
    "\n",
    "        predict_y = predict_y.astype(int)\n",
    "        res.append(predict_y)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def calc_jaccard(imgA, imgB):\n",
    "    \"\"\"calculate the jaccard score\"\"\"\n",
    "    num_A = len(np.unique(imgA))\n",
    "    num_B = len(np.unique(imgB))\n",
    "\n",
    "    if num_A < num_B:\n",
    "        i = imgA\n",
    "        imgA = imgB\n",
    "        imgB = i\n",
    "\n",
    "    unqA = np.unique(imgA)\n",
    "    for i in range(len(unqA)):\n",
    "        imgA[imgA == unqA[i]] = i\n",
    "\n",
    "    unqB = np.unique(imgB)\n",
    "    for i in range(len(unqB)):\n",
    "        imgB[imgB == unqB[i]] = i\n",
    "\n",
    "    hit_matrix = np.zeros([unqA.size, unqB.size])\n",
    "\n",
    "    for i in range(1, unqA.size):\n",
    "        A_chan = (imgA == i)\n",
    "        for j in range(1, unqB.size):\n",
    "            B_chan = (imgB == j)\n",
    "            A_and_B = A_chan * B_chan\n",
    "            B_chan[A_chan == 1] = 1\n",
    "            hit_matrix[i, j] = np.sum(A_and_B) / np.sum(B_chan)\n",
    "\n",
    "    jaccard_list = []\n",
    "    for j in range(1, unqB.size):\n",
    "        jac_col = np.max(hit_matrix[:, j])\n",
    "        jaccard_list.append(jac_col)\n",
    "\n",
    "    j_score = np.sum(jaccard_list) / max(num_A, num_B)\n",
    "    \"\"\"\n",
    "    print(\"A, B:\", num_A, num_B, \"max:\", max(jaccard_list),\n",
    "          \"min:\", min(jaccard_list), \"mean:\", j_score)\n",
    "    print(jaccard_list)\n",
    "    \"\"\"\n",
    "\n",
    "    return j_score\n",
    "\n",
    "def show_pic(picA, picB, picC):\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"x\")\n",
    "    plt.imshow(picA, cmap='gray')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"GT\")\n",
    "    plt.imshow(picB)\n",
    "\n",
    "    if picC is not None:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Predict\")\n",
    "        plt.imshow(picC)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "epochs = 100\n",
    "epoch_lapse = 5\n",
    "threshold = 0.5\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################################################################| 79/79 [00:16<00:00,  4.93it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.20it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.17it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.16it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.11it/s]\n",
      "  0%|                                                                                            | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 5.000000 : 4.261444 and validation loss : 0.480019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.14it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.11it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.15it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.12it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.05it/s]\n",
      "  0%|                                                                                            | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 10.000000 : 3.233885 and validation loss : 0.563821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.14it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.14it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.09it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.07it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.01it/s]\n",
      "  0%|                                                                                            | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 15.000000 : 2.601982 and validation loss : 0.563295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.19it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.12it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.01it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.15it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.00it/s]\n",
      "  0%|                                                                                            | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 20.000000 : 2.311662 and validation loss : 0.551328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.14it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.09it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.10it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.09it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.01it/s]\n",
      "  0%|                                                                                            | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 25.000000 : 2.427837 and validation loss : 0.590202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.14it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.09it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.05it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.00it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.12it/s]\n",
      "  0%|                                                                                            | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 30.000000 : 1.830118 and validation loss : 0.578145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.16it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.09it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.09it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  4.99it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.06it/s]\n",
      "  0%|                                                                                            | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 35.000000 : 1.950086 and validation loss : 0.609834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.19it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.13it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.05it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.10it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.10it/s]\n",
      "  0%|                                                                                            | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 40.000000 : 1.605719 and validation loss : 0.610865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.14it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.11it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.09it/s]\n",
      "100%|###################################################################################| 79/79 [00:15<00:00,  5.04it/s]\n",
      " 67%|#######################################################6                           | 53/79 [00:10<00:05,  5.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b2f83092a494>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mbatch_train_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_train_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mbatch_train_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_train_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         batch_loss = train_step(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_iter = np.ceil(x_train.shape[0] / batch_size).astype(int)\n",
    "\n",
    "for _ in range(20):\n",
    "    total_loss = 0\n",
    "    for i in tqdm.tqdm(range(epoch_iter), ascii=True, ncols=120):\n",
    "        batch_train_x = torch.from_numpy(\n",
    "            x_train[i * batch_size: (i + 1) * batch_size]).float()\n",
    "        batch_train_y = torch.from_numpy(\n",
    "            y_train[i*batch_size:(i + 1)*batch_size]).long()\n",
    "\n",
    "        if (len(batch_train_x.size()) == 3):\n",
    "            batch_train_x = batch_train_x.unsqueeze(1)\n",
    "        if (len(batch_train_y.size()) == 3):\n",
    "            batch_train_y = batch_train_y.unsqueeze(1)\n",
    "\n",
    "        if use_gpu:\n",
    "            batch_train_x = batch_train_x.cuda()\n",
    "            batch_train_y = batch_train_y.cuda()\n",
    "        batch_loss = train_step(\n",
    "            batch_train_x, batch_train_y, optimizer, criterion, unet, width_out, height_out)\n",
    "        total_loss += batch_loss\n",
    "    # print(\"i:\", _, \"total_loss:\", total_loss.cpu().item())\n",
    "    \n",
    "    if (_+1) % 5 == 0:\n",
    "        val_loss = get_val_loss(x_val, y_val, width_out, height_out, unet, batch_size)\n",
    "        print(\"Total loss in epoch %f : %f and validation loss : %f\" %\n",
    "              (_+1, total_loss, val_loss))\n",
    "    \n",
    "    continue\n",
    "import datetime\n",
    "filename = 'unet-' + datetime.datetime.now().strftime('%Y%m%d%H%M%S') +'.pth'\n",
    "torch.save(unet.state_dict(), 'C:/me/test/save/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
